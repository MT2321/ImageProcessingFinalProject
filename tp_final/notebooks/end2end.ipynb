{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Rectangular Box Extraction\n",
    "Applying classical image processing methods such as Canny filter, dilation and erotion and Houghes Transform to extract the main rectangular shapes from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing.BoxDetector import BroadBoxDetector\n",
    "from image_processing.BoxDetectorUtils import BoxDetectorUtils\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "root = \"../../assets/imgs\"\n",
    "individual_imgs_results_dir =  \"../../results/individual_spots\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "We us the top layer, without the classsification head, of a vgg model to map the images to a feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mrtbuntu/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(500)\n",
    "def extract_features(image_path):\n",
    "    \"\"\" Given the image path, apply preprocessing and compute the feature vector\"\"\"\n",
    "    input_image = Image.open(image_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    return output[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67df1fb171624706b80086bfe9914bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = os.listdir(root) # raw video thumbnails\n",
    "results_df = pd.DataFrame(columns=[\"src_image\",\"dst_image\", \"box\", \"feature_vector\"])\n",
    "\n",
    "for src_image in tqdm(images):\n",
    "    image_path = f\"{root}/{src_image}\"\n",
    "    pixels = plt.imread(image_path)\n",
    "    BBD = BroadBoxDetector()\n",
    "    bounding_boxes = BBD.detect_boxes(pixels, find_zocalos=False)\n",
    "\n",
    "    for counter, bounding_box in enumerate(bounding_boxes):\n",
    "        individual_spot = BoxDetectorUtils.crop_frame(pixels, bounding_box) # cajita individual\n",
    "        individual_name = f\"{src_image.split('.')[0]}_{counter}.jpg\"\n",
    "\n",
    "        individual_box_path = f\"{individual_imgs_results_dir}/{individual_name}\"\n",
    "        plt.imsave(individual_box_path, individual_spot)\n",
    "\n",
    "        vgg11_features = extract_features(individual_box_path)\n",
    "        bgr_hist = BoxDetectorUtils.get_bgr_hist(individual_spot, bins=32)\n",
    "\n",
    "        feature_vector = vgg11_features #np.concatenate((vgg11_features, bgr_hist))\n",
    "        # feature_vector = np.concatenate((vgg11_features, bgr_hist))\n",
    "        data = {\n",
    "            \"src_image\":src_image,\n",
    "            \"dst_image\":individual_name,\n",
    "            \"box\": bounding_box,\n",
    "            \"feature_vector\": feature_vector\n",
    "        }\n",
    "        results_df = results_df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"results.pck\"\n",
    "dataset_name  = \"results_vgg16BGR2.pck\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(f\"../../results/{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(f\"../../results/{dataset_name}\")\n",
    "results_df = results_df[results_df['src_image']!='betsson.jpeg']\n",
    "src_image_column = results_df['src_image'].map(lambda name: int(name.split(\".\")[0]))\n",
    "results_df['src_image_number'] = src_image_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_image</th>\n",
       "      <th>dst_image</th>\n",
       "      <th>box</th>\n",
       "      <th>feature_vector</th>\n",
       "      <th>src_image_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0_0.jpg</td>\n",
       "      <td>(356, 327, 151, 104)</td>\n",
       "      <td>[-2.6547847, -0.8471976, -3.809546, -2.3656282...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0_1.jpg</td>\n",
       "      <td>(202, 326, 151, 105)</td>\n",
       "      <td>[-2.5192688, -2.3182557, -3.2193217, -2.103216...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1_0.jpg</td>\n",
       "      <td>(364, 331, 152, 103)</td>\n",
       "      <td>[-0.7869164, 0.036874715, -0.5789321, 0.159186...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1_1.jpg</td>\n",
       "      <td>(193, 332, 152, 102)</td>\n",
       "      <td>[-1.8711119, -1.6433448, -1.005236, -0.1895348...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.jpg</td>\n",
       "      <td>100_0.jpg</td>\n",
       "      <td>(347, 301, 160, 108)</td>\n",
       "      <td>[-2.7626224, -0.7975287, -2.7019863, -3.088944...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>96.jpg</td>\n",
       "      <td>96_0.jpg</td>\n",
       "      <td>(187, 302, 160, 107)</td>\n",
       "      <td>[-1.4451208, -1.5924901, 0.5326917, -1.4767649...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>96.jpg</td>\n",
       "      <td>96_1.jpg</td>\n",
       "      <td>(363, 301, 160, 108)</td>\n",
       "      <td>[-1.9138888, -1.0630454, 0.69642, -1.2709609, ...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>98.jpg</td>\n",
       "      <td>98_0.jpg</td>\n",
       "      <td>(288, 284, 134, 91)</td>\n",
       "      <td>[-1.9513379, -1.0318924, -0.28411716, 1.256894...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>99.jpg</td>\n",
       "      <td>99_0.jpg</td>\n",
       "      <td>(363, 301, 160, 108)</td>\n",
       "      <td>[-0.72070426, -1.5725075, -1.9164596, -1.16294...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>99.jpg</td>\n",
       "      <td>99_1.jpg</td>\n",
       "      <td>(188, 301, 159, 108)</td>\n",
       "      <td>[-2.413773, -0.5047608, -3.4968288, -2.9554837...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    src_image  dst_image                   box  \\\n",
       "0       0.jpg    0_0.jpg  (356, 327, 151, 104)   \n",
       "1       0.jpg    0_1.jpg  (202, 326, 151, 105)   \n",
       "2       1.jpg    1_0.jpg  (364, 331, 152, 103)   \n",
       "3       1.jpg    1_1.jpg  (193, 332, 152, 102)   \n",
       "4     100.jpg  100_0.jpg  (347, 301, 160, 108)   \n",
       "..        ...        ...                   ...   \n",
       "515    96.jpg   96_0.jpg  (187, 302, 160, 107)   \n",
       "516    96.jpg   96_1.jpg  (363, 301, 160, 108)   \n",
       "517    98.jpg   98_0.jpg   (288, 284, 134, 91)   \n",
       "518    99.jpg   99_0.jpg  (363, 301, 160, 108)   \n",
       "519    99.jpg   99_1.jpg  (188, 301, 159, 108)   \n",
       "\n",
       "                                        feature_vector  src_image_number  \n",
       "0    [-2.6547847, -0.8471976, -3.809546, -2.3656282...                 0  \n",
       "1    [-2.5192688, -2.3182557, -3.2193217, -2.103216...                 0  \n",
       "2    [-0.7869164, 0.036874715, -0.5789321, 0.159186...                 1  \n",
       "3    [-1.8711119, -1.6433448, -1.005236, -0.1895348...                 1  \n",
       "4    [-2.7626224, -0.7975287, -2.7019863, -3.088944...               100  \n",
       "..                                                 ...               ...  \n",
       "515  [-1.4451208, -1.5924901, 0.5326917, -1.4767649...                96  \n",
       "516  [-1.9138888, -1.0630454, 0.69642, -1.2709609, ...                96  \n",
       "517  [-1.9513379, -1.0318924, -0.28411716, 1.256894...                98  \n",
       "518  [-0.72070426, -1.5725075, -1.9164596, -1.16294...                99  \n",
       "519  [-2.413773, -0.5047608, -3.4968288, -2.9554837...                99  \n",
       "\n",
       "[520 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering y clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los feature vectors podemos correr un algoritmo de clasificación que nos permita agruparlas por similitud coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "classifier = NearestNeighbors(radius=55)\n",
    "X = results_df[\"feature_vector\"].to_numpy()\n",
    "X = np.array([item for item in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(radius=55)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos las predicciones y guardamos las publicidades similares en carpetas individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_differents(distance, nbrs, threshold):\n",
    "    new_nbrs = []\n",
    "    for d, n in zip(distance, nbrs):\n",
    "        if d<threshold:\n",
    "            new_nbrs.append(n)\n",
    "    return new_nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "classes_img_names = []\n",
    "images_already_classified = []\n",
    "for counter, feature_vector in enumerate(X):\n",
    "    if counter not in images_already_classified:\n",
    "        rng = classifier.radius_neighbors([feature_vector], return_distance=True, sort_results=True)\n",
    "        # nbrs = filter_differents(distance[0], nbrs, 30)\n",
    "        # print(nbrs, distance)\n",
    "        # print(np.asarray(rng[0][0]))\n",
    "        nbrs = np.asarray(rng[1][0])\n",
    "        classes.append(nbrs)\n",
    "        tmp = []\n",
    "        for c in nbrs:\n",
    "            images_already_classified.append(c)\n",
    "            tmp.append(results_df.iloc[c,:].src_image)\n",
    "        classes_img_names.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_already_classified.count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.jpg',\n",
       " '278.jpg',\n",
       " '143.jpg',\n",
       " '287.jpg',\n",
       " '62.jpg',\n",
       " '318.jpg',\n",
       " '117.jpg',\n",
       " '57.jpg',\n",
       " '287.jpg',\n",
       " '0.jpg',\n",
       " '214.jpg',\n",
       " '310.jpg',\n",
       " '310.jpg',\n",
       " '45.jpg',\n",
       " '259.jpg',\n",
       " '263.jpg',\n",
       " '288.jpg',\n",
       " '32.jpg',\n",
       " '318.jpg',\n",
       " '63.jpg',\n",
       " '117.jpg',\n",
       " '144.jpg',\n",
       " '311.jpg',\n",
       " '6.jpg',\n",
       " '58.jpg',\n",
       " '190.jpg',\n",
       " '282.jpg',\n",
       " '152.jpg',\n",
       " '1.jpg',\n",
       " '152.jpg',\n",
       " '270.jpg',\n",
       " '314.jpg',\n",
       " '96.jpg',\n",
       " '56.jpg',\n",
       " '87.jpg',\n",
       " '143.jpg',\n",
       " '247.jpg',\n",
       " '148.jpg',\n",
       " '151.jpg',\n",
       " '24.jpg',\n",
       " '319.jpg']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_img_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter, src_images in enumerate(classes_img_names):\n",
    "    imgs = [plt.imread(f\"{root}/{src_image}\") for src_image in src_images]\n",
    "    stacked = np.hstack(imgs)\n",
    "    try:\n",
    "        plt.imsave(f\"../../results/clustered_onlyVGG16BGR/{counter}.jpg\", stacked)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0387bbc81de27c565f4d9105f593e16542669bf817df81f684616723e155003"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('graph-match')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
